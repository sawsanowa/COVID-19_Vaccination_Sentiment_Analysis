{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-Data Cleaning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7ZNFCf-u9gh"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59hj9uOvmdde"
      },
      "source": [
        "#-- used lib --#\n",
        "import tweepy as tw         \n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "from nltk.corpus import stopwords\n",
        "import sys\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#--Not used--#\n",
        "from google.colab import drive  # to mount Drive to Colab notebook\n",
        "import json\n",
        "from csv import writer\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "import time\n",
        "import oauth2client\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn2Um3lQXEf_"
      },
      "source": [
        "df=pd.read_csv('/content/Final_ALL_Labled_012.csv', usecols=['full_text'])\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfzLJeKpbNww"
      },
      "source": [
        "**Cleaning Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HauilOWqTq_x"
      },
      "source": [
        "First used way to clean the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD8XBvIVGJxS"
      },
      "source": [
        "def clean_tweets(text):\n",
        "  text=re.sub(\"RT @[\\w]*:\",\"\",text)\n",
        "  text=re.sub(\"@[\\w]*\",\"\",text)\n",
        "  text=re.sub(\"https?://[A-Za-z0-9./]*\",\"\",text)\n",
        "  text=re.sub(\"[0-9]\",\"\",text)\n",
        "  text=re.sub(\"[A-Z]\",\"\",text)\n",
        "  text=re.sub(\"[a-z]\",\"\",text)\n",
        "  #text=re.sub(\"_\",\" \",text) \n",
        "  text=re.sub(\"\\n\",\"\",text)\n",
        "  return text;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE6SIcR7RGX_"
      },
      "source": [
        "  df[\"full_text\"]=df[\"full_text\"].apply(lambda x : clean_tweets(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kk1lmtVRIKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2627e536-1977-4b11-d111-0c519e4a4a36"
      },
      "source": [
        "  print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             full_text\n",
            "0     بفضل الله قبل قليل وخلال ٩ دقائق فقط اخذت #لق...\n",
            "1    ️⃣ ألمانيا️⃣ فرنسا🔟 إيطاليا️⃣️⃣ بولندا️⃣️⃣ هول...\n",
            "2    #لقاح_كورونا  اليوم  مابدت الاعراض الا بعد  سا...\n",
            "3     قبل موعد أخذي الجُرعة الأولى من #لقاح_كورونا ...\n",
            "4     لاحل للخروج من الوباء والبلاء الا بالمطعوم حي...\n",
            "..                                                 ...\n",
            "816   مأساة أسرة.. وفاة الوالد والأم في حالة حرجة ب...\n",
            "817  #لقاح_كورونا ان برداً يسكن بين جوانحي ونارا تو...\n",
            "818  اخذت القاح بكتفي وش دخلي رجلي تهنق💔😣 #لقاح_كورونا\n",
            "819  حديث هذا الويك اند: فايزر ولا اكسفور؟!#لقاح_كو...\n",
            "820  بحسب ذمة الموقع ولمن يقول أن #لقاح_كورونا مؤام...\n",
            "\n",
            "[821 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByUcanLXUV2p"
      },
      "source": [
        "pip install pyarabic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BpBDtyQUZ-T"
      },
      "source": [
        "import pyarabic.araby as araby\n",
        "import pyarabic.number as number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0JvjYhXdPeJ"
      },
      "source": [
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyq7MSPqcdL7"
      },
      "source": [
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC2mPnZldy4U"
      },
      "source": [
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآ]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"و\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1FI5H2sepRA"
      },
      "source": [
        "df[\"full_text\"]=df[\"full_text\"].apply(lambda x : normalize_arabic(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EY8Lf5SdIMI"
      },
      "source": [
        "def remove_diacritics(text):\n",
        "    text = re.sub(arabic_diacritics,\"\", text)\n",
        "    return text;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PRY5aKQek2A"
      },
      "source": [
        "  df[\"full_text\"]=df[\"full_text\"].apply(lambda x : remove_diacritics(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pS44k1ideTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d45ba80-1acb-4e79-fe8a-7f941ace6854"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   full_text\n",
            "sentiment                                          full_text\n",
            "0          اتخيل بعض الناس وهو ياخذ لقاح كورونا حط فمك بل...\n",
            "0          لقاح كورونا رحاال هذا والله اللي يبي له لقاح ا...\n",
            "0          انتقل الي رحمه الله المهندس هزاع محمد ابو طالب...\n",
            "0          وفاه شاب ثلاثيني بحادث سياره بعد تلقيه لقاح كو...\n",
            "...                                                      ...\n",
            "1                       جرعه من لقاح فايروس كورونا تصل ديالي\n",
            "1             عداد متلقي لقاح كورونا في السعوديه احصاء مباشر\n",
            "1          لقاح كورونا اخويه امس اخذ اول جرعه من اللقاح م...\n",
            "1          لقاح كورونا سبق وجاني كورونا وتعافيت منه الحمد...\n",
            "1          الحمد لله تلقيت اليوم الجرعه الاولي من لقاح كو...\n",
            "\n",
            "[261 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU_pNQIffNzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "091fec80-239c-4e63-eed5-0862110f2b41"
      },
      "source": [
        "df['full_text'] = df['full_text'].str.replace(r'\\W',\" \")#Remove Special \"Char\" after removing the diacritics to avoid deleteing the text contains diacritics\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             full_text\n",
            "0     بفضل الله قبل قليل وخلال ٩ دقاءق فقط اخذت  لق...\n",
            "1       المانيا   فرنسا  ايطاليا     بولندا     هول...\n",
            "2     لقاح_كورونا  اليوم  مابدت الاعراض الا بعد  سا...\n",
            "3     قبل موعد اخذي الجرعه الاولي من  لقاح_كورونا ظ...\n",
            "4     لاحل للخروج من الوباء والبلاء الا بالمطعوم حي...\n",
            "..                                                 ...\n",
            "816   ماساه اسره   وفاه الوالد والام في حاله حرجه ب...\n",
            "817   لقاح_كورونا ان بردا يسكن بين جوانحي ونارا توق...\n",
            "818  اخذت القاح بكتفي وش دخلي رجلي تهنق    لقاح_كورونا\n",
            "819  حديث هذا الويك اند  فايزر ولا اكسفور   لقاح_كو...\n",
            "820  بحسب ذمه الموقع ولمن يقول ان  لقاح_كورونا موام...\n",
            "\n",
            "[821 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monXSGb6_AwQ"
      },
      "source": [
        "#df['full_text'] = df['full_text'].str.replace(r'[0-9]',\"\") #Remove numbers\n",
        "#print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm8x8Rvj_PNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881683c3-fa2e-442b-b62a-0a8c485e5a1c"
      },
      "source": [
        "df['full_text'] = df['full_text'].str.replace(r'_',\" \") #Remove numbers\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             full_text\n",
            "0     بفضل الله قبل قليل وخلال ٩ دقاءق فقط اخذت  لق...\n",
            "1       المانيا   فرنسا  ايطاليا     بولندا     هول...\n",
            "2     لقاح كورونا  اليوم  مابدت الاعراض الا بعد  سا...\n",
            "3     قبل موعد اخذي الجرعه الاولي من  لقاح كورونا ظ...\n",
            "4     لاحل للخروج من الوباء والبلاء الا بالمطعوم حي...\n",
            "..                                                 ...\n",
            "816   ماساه اسره   وفاه الوالد والام في حاله حرجه ب...\n",
            "817   لقاح كورونا ان بردا يسكن بين جوانحي ونارا توق...\n",
            "818  اخذت القاح بكتفي وش دخلي رجلي تهنق    لقاح كورونا\n",
            "819  حديث هذا الويك اند  فايزر ولا اكسفور   لقاح كو...\n",
            "820  بحسب ذمه الموقع ولمن يقول ان  لقاح كورونا موام...\n",
            "\n",
            "[821 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMWMUuUU_kG1"
      },
      "source": [
        "#df['full_text'] = df['full_text'].str.replace(r'[A-Z]',\"\") #Remove English Capital letters\n",
        "#print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzZsp1fg9XF_"
      },
      "source": [
        "df.to_csv('AllClean14-3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bsvL2OG8Byj",
        "outputId": "2a729895-e361-4d23-af40-e2f057373a24"
      },
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQf8FHVF8UwP"
      },
      "source": [
        "arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mq0Csj4UnQD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "3c1c95a1-76cb-406e-9023-a596e4fff659"
      },
      "source": [
        "def tokenization(text):\n",
        "    text = re.split('\\W+', text)\n",
        "    return text\n",
        "\n",
        "df['full_text_tokenized'] = df['full_text'].apply(lambda x: tokenization(x.lower()))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "      <th>full_text_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بحمد الله تلقيت اليوم الجرعه الاولى من لقاح كو...</td>\n",
              "      <td>[بحمد, الله, تلقيت, اليوم, الجرعه, الاولى, من,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>اتخيل بعض الناس وهو ياخذ لقاح كورونا حط فمك بل...</td>\n",
              "      <td>[اتخيل, بعض, الناس, وهو, ياخذ, لقاح, كورونا, ح...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>سوف تنتهي ازمه كورونا باذن الله وتكلل كل الجهو...</td>\n",
              "      <td>[سوف, تنتهي, ازمه, كورونا, باذن, الله, وتكلل, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>الحمد لله اخذت اليوم الثلاثاء الجرعه الاولى من...</td>\n",
              "      <td>[الحمد, لله, اخذت, اليوم, الثلاثاء, الجرعه, ال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>بشكل عام لا تتوفر ادله علميه كافيه تفيد بتمام ...</td>\n",
              "      <td>[بشكل, عام, لا, تتوفر, ادله, علميه, كافيه, تفي...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           full_text                                full_text_tokenized\n",
              "0  بحمد الله تلقيت اليوم الجرعه الاولى من لقاح كو...  [بحمد, الله, تلقيت, اليوم, الجرعه, الاولى, من,...\n",
              "1  اتخيل بعض الناس وهو ياخذ لقاح كورونا حط فمك بل...  [اتخيل, بعض, الناس, وهو, ياخذ, لقاح, كورونا, ح...\n",
              "2  سوف تنتهي ازمه كورونا باذن الله وتكلل كل الجهو...  [سوف, تنتهي, ازمه, كورونا, باذن, الله, وتكلل, ...\n",
              "3  الحمد لله اخذت اليوم الثلاثاء الجرعه الاولى من...  [الحمد, لله, اخذت, اليوم, الثلاثاء, الجرعه, ال...\n",
              "4  بشكل عام لا تتوفر ادله علميه كافيه تفيد بتمام ...  [بشكل, عام, لا, تتوفر, ادله, علميه, كافيه, تفي..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}